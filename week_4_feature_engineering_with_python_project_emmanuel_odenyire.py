# -*- coding: utf-8 -*-
"""Week 4 Feature Engineering with Python Project- Emmanuel Odenyire.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WAZSSXbNOoMD0fFWeBsyh83GZfdomVty

#Feature Engineering with Python Project

###Defining the Research Question

####Background

Logistics in Sub-Saharan Africa increases the cost of manufactured goods by up to 320%; while in Europe, it only accounts for up to 90% of the manufacturing cost. Sendy is a business-to-business platform established in 2014, to enable businesses of all types and sizes to transport goods more efficiently across East Africa. The company is headquartered in Kenya with a team of more than 100 staff, focused on building practical solutions for Africa’s dynamic transportation needs, from developing apps and web solutions to providing dedicated support for goods on the move.

####Problem Statement

Sendy has hired you to help predict the estimated time of delivery of orders, from the point of driver pickup to the point of arrival at the final destination. Build a model that predicts an accurate delivery time, from picking up a package arriving at the final destination. An accurate arrival time prediction will help all business to improve their logistics and communicate the accurate time their time to their customers. You will be required to perform various feature engineering techniques while preparing your data for further analysis.

###Data Importation

####Import important Libraries
"""

# Importing the neccesary libraries needed
import pandas as pd
import seaborn as sns
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import OrdinalEncoder
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error
from sklearn.svm import SVR 
from sklearn.neighbors import KNeighborsRegressor
from sklearn.decomposition import PCA

# Dataset URL = https://bit.ly/3deaKEM
# import the data and preview

features_df = pd.read_csv("https://bit.ly/3deaKEM")

pd.set_option('display.max_columns', None)

features_df.head()

"""###Data Exploration"""

# Check the dataset size
features_df.shape

# Check for NaN values 
features_df.isna().sum()

# Summary stats for the data set
features_df.describe()

"""###Data Cleaning"""

# standardize column names to lower case and strip leading and trailing whitespaces
features_df.columns = features_df.columns.str.lower().str.strip()
features_df.columns

# Handing NaN values
'''
We notice that (Temperature 4366) and (Precipitation in millimeters  20649) are the only values with Missing data
To handle it, we Will drop 'Precipitation in millimeters' since most records are missing and does not significalty affect estimated delivery time
We will replace 'Temperature' with its mean 
'''
features_df.drop(["precipitation in millimeters"], axis=1, inplace=True)
features_df.fillna(features_df.mean(), inplace=True)

features_df.head()

# Check for nan which should now be zero
features_df.isna().sum()

"""###Data Analysis (Univariate and Bivariate)"""

# Univariate

features_df.describe()

# See how our variable correlate with the Y target value
# Plotting a correlation matrix
# ---
#
df_corr = features_df.corr()
plt.figure(figsize=(10,10))

# We then plot our heatmap visualistion
# 

sns.heatmap(df_corr, annot=True, linewidth=0.5, cmap='coolwarm');

"""###Data Preparation"""

# Drop columns that are not important for our models
features_df.head()

# Convert categorical data to numerical using ordinal encoding 

enc = OrdinalEncoder()
features_df[["order no","user id", "vehicle type", "personal or business", "rider id"]] = enc.fit_transform(
    features_df[["order no","user id", "vehicle type", "personal or business", "rider id"]])

features_df.head()

# placement time to time components 
features_df["placement_time_hour"] = pd.to_datetime(features_df["placement - time"], format="%I:%M:%S %p").dt.hour
features_df["placement_time_minute"] = pd.to_datetime(features_df["placement - time"], format="%I:%M:%S %p").dt.minute
features_df["placement_time_second"] = pd.to_datetime(features_df["placement - time"], format="%I:%M:%S %p").dt.second

# confirmation - time to time components 
features_df["confirmation_time_hour"] = pd.to_datetime(features_df["confirmation - time"], format="%I:%M:%S %p").dt.hour
features_df["confirmation_time_minute"] = pd.to_datetime(features_df["confirmation - time"], format="%I:%M:%S %p").dt.minute
features_df["confirmation_time_second"] = pd.to_datetime(features_df["confirmation - time"], format="%I:%M:%S %p").dt.second

# arrival at pickup - time to time components 
features_df["arrival at pickup_time_hour"] = pd.to_datetime(features_df["arrival at pickup - time"], format="%I:%M:%S %p").dt.hour
features_df["arrival at pickup_time_minute"] = pd.to_datetime(features_df["arrival at pickup - time"], format="%I:%M:%S %p").dt.minute
features_df["arrival at pickup_time_second"] = pd.to_datetime(features_df["arrival at pickup - time"], format="%I:%M:%S %p").dt.second

# pickup - time to time components 
features_df["pickup_time_hour"] = pd.to_datetime(features_df["pickup - time"], format="%I:%M:%S %p").dt.hour
features_df["pickup_time_minute"] = pd.to_datetime(features_df["pickup - time"], format="%I:%M:%S %p").dt.minute
features_df["pickup_time_second"] = pd.to_datetime(features_df["pickup - time"], format="%I:%M:%S %p").dt.second

# arrival at destination - time to time components 
features_df["arrival at destination_time_hour"] = pd.to_datetime(features_df["arrival at destination - time"], format="%I:%M:%S %p").dt.hour
features_df["arrival at destination_time_minute"] = pd.to_datetime(features_df["arrival at destination - time"], format="%I:%M:%S %p").dt.minute
features_df["arrival at destination_time_second"] = pd.to_datetime(features_df["arrival at destination - time"], format="%I:%M:%S %p").dt.second

features_df.head()

# Drop time columns since new columns have been derived from them
features_df.drop(["placement - time", "confirmation - time", "arrival at pickup - time","pickup - time","arrival at destination - time"], inplace=True, axis = 1)

features_df.head()

"""####Feature construction"""

# Create new features 
# speed = distance/time

features_df["speed"] = features_df["distance (km)"]/features_df["time from pickup to arrival"]

# manhattan distance
features_df["manhattan distance"] = abs(features_df["destination lat"] - features_df["pickup lat"]) + abs(features_df["destination long"] - features_df["pickup long"])

# haversine distance
def haversine_vectorize(lon1, lat1, lon2, lat2):

    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])

    newlon = lon2 - lon1
    newlat = lat2 - lat1

    haver_formula = np.sin(newlat/2.0)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(newlon/2.0)**2

    dist = 2 * np.arcsin(np.sqrt(haver_formula ))
    km = 6367 * dist #6367 for distance in KM 
    return km

features_df['haversine_dist'] = haversine_vectorize(features_df['pickup long'],features_df['pickup lat'],features_df['destination long'],features_df['destination lat'])

features_df.head()

# bearing
import math
def get_bearing(lat1,lon1,lat2,lon2):
    dLon = lon2 - lon1;
    y = np.sin(dLon) * np.cos(lat2);
    x = np.cos(lat1)*np.sin(lat2) - np.sin(lat1)*np.cos(lat2)*np.cos(dLon);
    brng = np.rad2deg(np.arctan2(y, x));

    return brng

features_df['bearing'] = get_bearing(features_df['pickup lat'],features_df['pickup long'],features_df['destination lat'],features_df['destination long'])
features_df.head()

"""###Data Modeling"""

# Feature Selection ○ Filter methods ○ Feature transformation (PCA, LDA, etc) ○ Wrapper methods

# Feature Selection 
# We will use this as as our base for our solution, then perform feature engineering 
# by filter methods.
# get X and Y (ie features and target) and split the data to train and validation datasets
X = features_df.drop(["time from pickup to arrival"], axis = 1)
Y = features_df["time from pickup to arrival"]

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size= 0.25, random_state=1234)

norm = MinMaxScaler().fit(X_train) 
X_train = norm.transform(X_train) 
X_test = norm.transform(X_test)

svm_regressor = SVR(kernel='rbf', C=10)
knn_regressor = KNeighborsRegressor()
dec_regressor = DecisionTreeRegressor(random_state=27)

svm_regressor.fit(X_train, Y_train)
knn_regressor.fit(X_train, Y_train)
dec_regressor.fit(X_train, Y_train)

# Making Predictions  
svm_y_pred = svm_regressor.predict(X_test)
knn_y_pred = knn_regressor.predict(X_test)
dec_y_pred = dec_regressor.predict(X_test)

# Finally, evaluating our models  
print(f'SVM RMSE: {mean_squared_error(Y_test, svm_y_pred, squared= False)}')
print(f'KNN RMSE: {mean_squared_error(Y_test, knn_y_pred, squared= False)}')
print(f'Decision Tree RMSE: {mean_squared_error(Y_test, dec_y_pred, squared= False)}')

"""####Filter methods: Pearson's Correlation Coefficient"""

# Pearson's Correlation Coefficient
# appply filter methods by plotting a correlation matrix
# ---
#
df_corr = features_df.corr()
plt.figure(figsize=(35,35))

# We then plot our heatmap visualistion
# 
import seaborn as sns
sns.heatmap(df_corr, annot=True, linewidth=0.5, cmap='coolwarm');

# We resolve to drop columns that have a weaker correlation to our target variable
          # vehicle type	
          # pickup_time_second
          # arrival at pickup_time_second	
          # confirmation_time_second
          # placement_time_minute
          # placement_time_hour
# Feature Selection 

# get X and Y (ie features and target) and split the data to train and validation datasets
X = features_df.drop(["time from pickup to arrival", "vehicle type","pickup_time_second",
             "arrival at pickup_time_second","confirmation_time_second","placement_time_minute","placement_time_hour" ], axis = 1)
Y = features_df["time from pickup to arrival"]

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size= 0.25, random_state=1234)

norm = MinMaxScaler().fit(X_train) 
X_train = norm.transform(X_train) 
X_test = norm.transform(X_test)

svm_regressor = SVR(kernel='rbf', C=10)
knn_regressor = KNeighborsRegressor()
dec_regressor = DecisionTreeRegressor(random_state=27)

svm_regressor.fit(X_train, Y_train)
knn_regressor.fit(X_train, Y_train)
dec_regressor.fit(X_train, Y_train)

# Making Predictions  
svm_y_pred = svm_regressor.predict(X_test)
knn_y_pred = knn_regressor.predict(X_test)
dec_y_pred = dec_regressor.predict(X_test)

# Finally, evaluating our models  
print(f'SVM RMSE: {mean_squared_error(Y_test, svm_y_pred, squared= False)}')
print(f'KNN RMSE: {mean_squared_error(Y_test, knn_y_pred, squared= False)}')
print(f'Decision Tree RMSE: {mean_squared_error(Y_test, dec_y_pred, squared= False)}')

"""**Obervations:**

**base model:**

SVM RMSE: 858.5075953284493

KNN RMSE: 913.675214686524

Decision Tree RMSE: 100.38431715151265

**Filter method:**

SVM RMSE: 850.9263059216776

KNN RMSE: 888.33899600557

Decision Tree RMSE: 105.83564047946668

####Wrapper Method: Step Forward Feature Selection
"""

# We'll need import and install the following packages: six, sys, mlrose and joblib
# to use `SequentialFeatureSelector` for feature selection from mlxtend.
# ---
#

# importing six and sys
import six
import sys
sys.modules['sklearn.externals.six'] = six

# installing mlrose
!pip install mlrose
import mlrose

# importing joblib
import joblib
sys.modules['sklearn.externals.joblib'] = joblib

# get X and Y (ie features and target) and split the data to train and validation datasets
X = features_df.drop(["time from pickup to arrival"], axis = 1)
Y = features_df["time from pickup to arrival"]

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size= 0.25, random_state=1234)

from mlxtend.feature_selection import SequentialFeatureSelector
feature_selector = SequentialFeatureSelector(dec_regressor,
           k_features=30,
           forward=True,
           verbose=2,
           scoring='r2',
           cv=4)
 
 # Perform step forward feature selection
feature_selector = feature_selector.fit(X_train, Y_train)

# Which are the selected features?
# The columns at these indexes are those which were selected
# ---
#
feat_cols = list(feature_selector.k_feature_idx_)
print(feat_cols)

# We can now use those features to build our model
# ---
# 

# Without step forward feature selection (sffs)
dec_regressor = DecisionTreeRegressor(random_state=27)
dec_regressor.fit(X_train, Y_train)

# With step forward feature selection
dec_regressor2 = DecisionTreeRegressor(random_state=27)
dec_regressor2.fit(X_train.iloc[:, feat_cols].values, Y_train)

# Making Predictions and determining the accuracies
y_test_pred = dec_regressor.predict(X_test)
print('Decision Tree RMSE Without sffs:', mean_squared_error(Y_test, y_test_pred, squared = False))

y_test_pred2 = dec_regressor2.predict(X_test.iloc[:, feat_cols].values)
print('Decision Tree RMSE with sffs:', mean_squared_error(Y_test, y_test_pred2, squared = False))

"""####Feature Transformation:Principal Component Analysis"""

# using the principal component analysis (PCA) to reduce our features into components.

# We select our features
# get X and Y (ie features and target) and split the data to train and validation datasets
X = features_df.drop(["time from pickup to arrival"], axis = 1)
Y = features_df["time from pickup to arrival"]

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size= 0.25, random_state=1234)

# Performing normalisation 
norm = MinMaxScaler().fit(X_train) 
X_train = norm.transform(X_train) 
X_test = norm.transform(X_test)

# Applying PCA
# ---
# NB: PCA relies the feature set and not the label data.
# ---
# 

pca = PCA()
X_train = pca.fit_transform(X_train)
X_test = pca.transform(X_test)

# Fitting in our models   
svm_regressor = SVR(kernel='rbf', C=10)
knn_regressor = KNeighborsRegressor()
dec_regressor = DecisionTreeRegressor(random_state=27)

svm_regressor.fit(X_train, Y_train)
knn_regressor.fit(X_train, Y_train)
dec_regressor.fit(X_train, Y_train)

# Making Predictions  
svm_y_pred = svm_regressor.predict(X_test)
knn_y_pred = knn_regressor.predict(X_test)
dec_y_pred = dec_regressor.predict(X_test)

# Finally, evaluating our models 
print('SVM RMSE:', mean_squared_error(Y_test, svm_y_pred, squared=False))
print('KNN RMSE:', mean_squared_error(Y_test, knn_y_pred, squared=False))
print('Decision Tree RMSE:', mean_squared_error(Y_test, dec_y_pred, squared=False))

"""###Model Evaluation"""

# To understand if our model(s) is working well with new data, we can leverage a number of evaluation metrics.


# The most popular metrics for measuring classification performance include 
# accuracy, precision, confusion matrix, log-loss, and AUC (area under the ROC curve).

# accuracy

"""###Challenging your Solution

challenge the solution

###Recommendations / Conclusion

provide recommendations and conclusions
"""